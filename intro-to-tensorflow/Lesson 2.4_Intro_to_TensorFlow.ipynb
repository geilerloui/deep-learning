{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1:\n",
    "What I should know:\n",
    "* Ran operations in ```tf.Session```.\n",
    "* Created a constant tensor with ```tf.constant()```.\n",
    "* Used ```tf.placeholder()``` and ```feed_dict``` to get input.\n",
    "* Applied the ```tf.add()```, ```tf.subtract()```, ```tf.multiply()```, and ```tf.divide()``` functions using numeric data.\n",
    "* Learned about casting between types with ```tf.cast()```\n",
    "\n",
    "## Hello, world tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "In TensorFlow, data isn't stored as integers, floats or strings. These values are encapsulated in an object called a tensor. In the case of ```hello_constant = tf.constant('Hello World!')```, ```hello_constant``` is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    "# C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.constant()``` is one of many TensorFlow operations you will use in this lesson. The tensor returned by ```tf.constant()``` is called a constant tensor, because the value of the tensor never changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum  [5. 5. 5.]\n",
      "diff  [1. 1. 1.]\n",
      "prod  [6. 6. 6.]\n",
      "quot  [1.5 1.5 1.5]\n"
     ]
    }
   ],
   "source": [
    "# It performs element-wise arithmetic\n",
    "a = tf.constant([3., 3., 3.])\n",
    "b = tf.constant([2., 2., 2.])\n",
    "\n",
    "suma = tf.add(a, b)                 # [ 5. 5. 5. ]\n",
    "diff = tf.subtract(a, b)           # [ 1. 1. 1. ]\n",
    "prod = tf.multiply(a, b)           # [ 6. 6. 6. ]\n",
    "quot = tf.divide(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    print(\"sum \", sess.run(suma))\n",
    "    print(\"diff \", sess.run(diff))\n",
    "    print(\"prod \", sess.run(prod))\n",
    "    print(\"quot \", sess.run(quot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "This operation will cast a tensor to a new type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
    "tf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:\n",
    "\n",
    "<img src=\"image/session.png\" style=\"height:200px\">\n",
    "\n",
    "A \"TensorFlow Session\", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has already created the tensor, hello_constant, from the previous lines. The next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code creates a session instance, sess, using tf.Session. The sess.run() function then evaluates the tensor and returns the results.\n",
    "\n",
    "## Placeholders and feed_dict\n",
    "\n",
    "A ```placeholder``` is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computational graph, without needing the data. In ```TensorFlow``` terminology, we then feed data into the graph through these placeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "# Statically typed (you can try shape=2 or 4 it will fail)\n",
    "x = tf.placeholder(dtype = \"float\", shape=3)\n",
    "# We allow x to take on any length\n",
    "x = tf.placeholder(dtype = \"float\", shape=None)\n",
    "\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We create a ```placeholder``` called ```x```, i.e. a place in memory where we will store value later on.\n",
    " Then, we create a Tensor called y, which is the operation of multiplying ```x``` by 2. Note that we haven’t defined any initial values for x yet.\n",
    " \n",
    "------------\n",
    "\n",
    "Placeholders can also have multiple dimensions, allowing for storing arrays. In the following example, we create a 3 by 2 matrix, and store some numbers in it. We then use the same operation as before to do element-wise doubling of the numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension of the placeholder is ```None```, meaning we can have any number of rows. The second dimension is fixed at 3, meaning each row needs to have three columns of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2:\n",
    "What I should know:\n",
    "* What is a Variable\n",
    "* How to initialize them\n",
    "    * In the session\n",
    "    * Outside of the session\n",
    "\n",
    "## Linear functions in TensorFlow\n",
    "\n",
    "As a reminder in a Neural Networks we can calculate a linear combination of inputs, weights and biasis as:\n",
    "$$\n",
    "y = xW +b\n",
    "$$\n",
    "\n",
    "### Weights and Bias in TensorFlow\n",
    "\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out ```tf.placeholder()``` and ```tf.constant()```, since those Tensors can't be modified. This is where ```tf.Variable``` class comes in.\n",
    "\n",
    "### tf.Variable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.Variable``` class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the ```tf.global_variables_initializer()``` function to initialize the state of all the Variable tensors.\n",
    "\n",
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.global_variables_initializer()``` call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the ```tf.Variable``` class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the ```tf.truncated_normal()``` function to generate random numbers from a normal distribution.\n",
    "\n",
    "### tf.truncated_normal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal(shape = (n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he ```tf.truncated_normal()``` function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "### tf.zeros()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tf.zeros(.)](https://www.tensorflow.org/api_docs/python/tf/zeros) function returns a tensor with all zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "sandbox.py et quiz.py  \n",
    "--> See file in the same folder\n",
    "\n",
    "## TensorFlow Softmax\n",
    "\n",
    "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.nn.softmax([2.0, 1.0, 0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as that! ```tf.nn.softmax()``` implements the softmax function for you. It takes in logits and returns softmax activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6590012  0.24243298 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "logit_data = [2.0, 1.0, 0.1]\n",
    "logits = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Calculate the softmax of the logits\n",
    "softmax = tf.nn.softmax(logits)    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # TODO: Feed in the logit data\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data} )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Cross Entropy\n",
    "\n",
    "To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "\n",
    "* ```tf.reduce_sum()```\n",
    "* ```tf.log()```\n",
    "\n",
    "### Reduce Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.reduce_sum()``` function takes an array of numbers and sums them together.\n",
    "\n",
    "### Natural Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.log(100.0)  # 4.60517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does exactly what you would expect it to do. ```tf.log()``` takes the natural log of a number.\n",
    "\n",
    "**Quiz**:  \n",
    "Print the cross entropy using softmax_data and one_hot_encod_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667497\n"
     ]
    }
   ],
   "source": [
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Implement cross entropy from session\n",
    "cross_entropy = - tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # TODO: Feed in the logit data\n",
    "    output = sess.run(cross_entropy, feed_dict={one_hot: one_hot_data,\n",
    "                                                softmax: softmax_data}\n",
    "                     )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
