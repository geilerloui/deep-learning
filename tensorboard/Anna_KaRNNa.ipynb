{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71, 22, 16,  5, 70, 62, 13, 52, 29, 69, 69, 69, 73, 16,  5,  5, 24,\n",
       "       52, 58, 16, 63, 53,  8, 53, 62, 55, 52, 16, 13, 62, 52, 16,  8,  8,\n",
       "       52, 16,  8, 53,  6, 62, 18, 52, 62, 49, 62, 13, 24, 52, 72,  7, 22,\n",
       "       16,  5,  5, 24, 52, 58, 16, 63, 53,  8, 24, 52, 53, 55, 52, 72,  7,\n",
       "       22, 16,  5,  5, 24, 52, 53,  7, 52, 53, 70, 55, 52, 15, 80,  7, 69,\n",
       "       80, 16, 24, 81, 69, 69,  0, 49, 62, 13, 24, 70, 22, 53,  7],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71, 22, 16,  5, 70, 62, 13, 52, 29, 69],\n",
       "       [79,  7, 60, 52, 22, 62, 52, 63, 15, 49],\n",
       "       [52,  1, 16, 70,  1, 22, 53,  7, 19, 52],\n",
       "       [15, 70, 22, 62, 13, 52, 80, 15, 72,  8],\n",
       "       [52, 70, 22, 62, 52,  8, 16,  7, 60, 51],\n",
       "       [52, 43, 22, 13, 15, 72, 19, 22, 52,  8],\n",
       "       [70, 52, 70, 15, 69, 60, 15, 81, 69, 69],\n",
       "       [15, 52, 22, 62, 13, 55, 62,  8, 58, 12],\n",
       "       [22, 16, 70, 52, 53, 55, 52, 70, 22, 62],\n",
       "       [62, 13, 55, 62,  8, 58, 52, 16,  7, 60]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    \n",
    "    seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                           name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4168 6.2078 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3666 5.8334 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.1489 5.2054 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.1603 5.2031 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.1621 5.2278 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 4.1278 5.2704 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 4.0642 5.3099 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 3.9952 5.2572 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 3.9303 5.2443 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 3.8797 5.3520 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 3.8366 5.3580 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.7992 5.3331 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.7631 5.4272 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.7327 5.5875 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.7043 5.4416 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.6784 5.5381 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.6549 5.6175 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.6350 5.5442 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.6161 5.5269 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.5969 5.5843 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.5806 5.6003 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.5652 5.6444 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.5506 5.5711 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.5373 5.5502 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.5245 5.5956 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.5130 5.5998 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.5025 5.5838 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.4917 6.3420 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.4816 5.7476 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.4724 5.6311 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.4647 5.5988 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.4563 5.6141 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.4481 5.6462 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.4409 5.6164 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.4335 5.6492 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.4269 5.6070 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.4201 5.5922 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.4134 5.5938 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.4069 5.6572 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.4010 5.5398 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.3951 5.6761 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.3897 5.6311 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.3841 5.7538 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.3790 5.5558 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.3738 5.6467 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.3693 5.6345 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.3650 5.6084 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.3610 5.6067 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.3570 5.6625 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.3532 6.3173 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.3493 6.7917 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.3454 6.1039 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.3419 5.5133 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.3381 6.0234 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.3347 5.9794 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.3309 6.1343 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.3276 5.5498 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.3245 5.7472 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.3213 5.5990 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.3183 5.7711 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.3153 5.6318 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.3127 6.5230 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.3104 6.4954 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.3074 6.3851 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.3045 6.6989 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.3022 7.3310 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.2998 8.2811 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.2968 6.5090 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.2941 6.9790 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.2919 6.5882 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.2895 6.4835 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.2874 6.4345 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.2851 6.5569 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.2828 8.7411 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.2808 7.3505 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.2789 6.6112 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.2768 5.9967 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.2748 6.8433 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.2727 6.9799 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.2705 6.0709 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.2685 6.1288 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.2666 6.5425 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.2648 7.5654 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.2628 7.2408 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.2608 6.9142 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.2587 7.1501 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.2568 7.2336 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.2548 7.3190 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.2532 7.0098 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.2514 7.4339 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.2498 7.4447 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.2483 6.7207 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.2469 6.4131 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.2456 6.6502 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.2443 6.4705 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.2429 6.4963 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.2416 7.0871 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.2401 7.5289 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.2387 8.5901 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.2372 8.0885 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.2357 5.8685 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.2344 5.8362 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.2330 7.0377 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.2315 6.4704 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.2301 6.7586 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.2287 6.5169 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.2271 6.3238 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.2255 6.2901 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.2241 6.5175 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.2224 6.3937 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.2208 6.3999 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.2193 6.1883 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.2177 6.1504 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.2161 7.0559 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.2144 6.7148 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.2127 7.7119 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.2110 7.6787 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.2095 6.6498 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.2079 7.7489 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.2063 8.0283 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.2049 11.1981 sec/batch\n",
      "Epoch 1/1  Iteration 122/178 Training loss: 3.2033 9.6608 sec/batch\n",
      "Epoch 1/1  Iteration 123/178 Training loss: 3.2017 6.7793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 124/178 Training loss: 3.2002 5.8994 sec/batch\n",
      "Epoch 1/1  Iteration 125/178 Training loss: 3.1985 7.7592 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.1966 7.1758 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.1950 6.2423 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.1934 8.1513 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.1916 6.7462 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.1899 8.1772 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.1882 6.7427 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
